{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEXT-TO-SPEECH WITH TACOTRON2\n",
    "\n",
    "The text-to-speech pipeline\n",
    "\n",
    "1. Text preprocessing\n",
    "* Text is encoded into a list of symbols\n",
    "\n",
    "2. Spectrogram generation\n",
    "* TacoTron2 model is used to generate a spectrogram from the encoded text\n",
    "\n",
    "3. Time-domain conversion\n",
    "* The spectrogram is then converted into a waveform. This process is called a `Vocoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 4.8] # Set default figsize\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text that will be read\n",
    "text = \"Hello world! Text to speech!\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Processing\n",
    "\n",
    "The pre-trained Tacotron2 model expects specific set of symbol tables.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phoneme-based encoding\n",
    "\n",
    "Similar to `Character based encoding`, but it uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "\n",
    "print(processed)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([processor.tokens[i] for i in processed[0, : lengths[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectrogram Generation\n",
    "\n",
    "TacoTron2 is used to generate the spectogram.\n",
    "\n",
    "`torchaudio.pipelines.Tacotron2TTSBundle` bundles the matching models and processors together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, _, _ = tacotron2.infer(processed, lengths)\n",
    "\n",
    "\n",
    "_ = plt.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(16, 4.3 * 3))\n",
    "for i in range(3):\n",
    "    with torch.inference_mode():\n",
    "        spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    print(spec[0].shape)\n",
    "    ax[i].imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Waveform Generation\n",
    "The last process is to get the waveform from the spectrogram.\n",
    "\n",
    "`torchaudio` provides vocoders based on `GriffinLim` and `WaveRNN`.\n",
    "\n",
    "1. WaveRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "text = \"Hello world! Text to speech!\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    waveforms, lengths = vocoder(spec, spec_lengths)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
    "ax1.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
    "ax2.plot(waveforms[0].cpu().detach())\n",
    "\n",
    "IPython.display.Audio(waveforms[0:1].cpu(), rate=vocoder.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Griffin-Lim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "waveforms, lengths = vocoder(spec, spec_lengths)\n",
    "\n",
    "fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 9))\n",
    "ax1.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
    "ax2.plot(waveforms[0].cpu().detach())\n",
    "\n",
    "IPython.display.Audio(waveforms[0:1].cpu(), rate=vocoder.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
